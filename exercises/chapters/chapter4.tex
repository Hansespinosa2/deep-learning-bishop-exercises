\section{Single-layer Networks: Regression}

\subsection{Exercise 4.1}
Showing the coefficients $\mathbf{w} = \{w_i\}$ that minimize the error function $E[\mathbf{w}] = \frac{1}{2} \sum_{n=1}^{N} (y(x_n,\mathbf{w}) - t_n)^2 $ are given by the solution to:
\begin{equation}
  \sum_{j=0}^{M}A_{ij}w_j = T_i
\end{equation}

where $A_{ij} = \sum_{n=1}^{N}x_n^{i+j}$ , $T_i = \sum_{n=1}^{N}x_n^it_n$ and $y(x,\mathbf{w}) = \sum_{j=0}^{M} w_j x^j$

\begin{gather}
  \frac{1}{2} \sum_{n=1}^{N} (y(x_n,\mathbf{w}) - t_n)^2 =  \frac{1}{2} \sum_{n=1}^{N} (\sum_{j=0}^{M} w_j x_n^j - t_n)^2 \\
  \frac{d}{dw_i} = \sum_{n=1}^{N} \sum_{j=0}^{M}( w_j x_n^j - t_n)x_n^i \\
  0 =  \sum_{j=0}^{M} w_j \sum_{n=1}^{N} x_n^{i+j} - x_n^it_n \\
  \sum_{n=1}^{N}x_n^it_n = \sum_{j=0}^{M} w_j \sum_{n=1}^{N} x_n^{i+j} \\
  T_i = \sum_{j=0}^{M}A_{ij}w_j
\end{gather}

\subsection{Exercise 4.2}
Not Attempted



\subsection{Exercise 4.3}
Showing that the $tanh$ function is related to the logistic sigmoid function by $tanh(a) = 2\sigma(2a) -1$
\begin{equation}
  tanh(a) = \frac{e^a-e^{-a}}{e^a+e^{-a}}
\end{equation}

\begin{equation}
  \sigma(a) = \frac{1}{1+e^{-a}}
\end{equation}

\begin{gather}
  \frac{e^a-e^{-a}}{e^a+e^{-a}} +1 = 2\sigma(2a) \\
  \frac{\frac{e^{2a} -1}{e^a} }{\frac{e^{2a} +1}{e^a}} +1 =2\sigma(2a) \\
  \frac{e^{2a}-1}{e^{2a}+1} + 1 = 2\sigma(2a) \\
  \frac{2e^{2a}}{e^{2a}+1} = 2\sigma(2a) \\
  \sigma(2a) = \frac{e^{2a}}{1+e^{2a}}
\end{gather}


Showing that $w_0 + \sum_{j=1}^{M}w_j\sigma(\frac{x-\mu_j}{s}) $ is a linear combination of $ u_0 + \sum_{j=1}^{M}u_j tanh(\frac{x-\mu_j}{2s})$

Defining $z_j = \frac{x-\mu_j}{s}$

\begin{gather}
  u_0 + \sum_{j=1}^{M}u_j tanh(\frac{z_j}{2}) \\ 
  = u_0 + -1 + 2\sum_{j=1}^{M}u_j \sigma(z_j)
\end{gather}

\begin{equation}
  y(x,\mathbf{u}) = u_0 -1 -2w_0 + 2y(x,\mathbf{w}) 
\end{equation}


\subsection{Exercise 4.4}
Not attempted

\subsection{Exercise 4.5}
The weighted sum of squares error function could attempt to weigh some data points more than others. This could tie to data-dependent noise variance by weighing the points that are outliers less than other points. The weighted part could also be a way to lower the weight of or completely negate the effect of duplicate data points. A statement could be set up where if a data point was duplicated, the weight will be set to 0.

Finding the solution $\mathbf{w^*}$ that minimizes 
\begin{equation}
  E_D(\mathbf{w}) = \frac{1}{2}\sum_{n=1}^{N}r_n(t_n - \mathbf{w}^T \mathbf{\phi}(\mathbf{x}_n))^2
\end{equation}

\begin{gather}
  \frac{d}{d\mathbf{w}} = -\sum_{n=1}^N r_n \phi_n (t_n-\mathbf{w}^T \phi_n) \\
  0 = \sum_{n=1}^{N} r_n \phi_n t_n - r_n \phi_n \mathbf{w}^T \phi_n \\
  \sum_{n=1}^{N} \mathbf{w}^T \phi_n =\sum_{n=1}^{N} t_n \\
  \mathbf{w} = (\Phi^T\Phi)^{-1}\Phi^T \mathbf{t}
\end{gather}
\subsection{Exercise 4.6}
Not attempted

\subsection{Exercise 4.7}
Finding $\Sigma_{ml}$:
\begin{equation}
  p(t|W,\Sigma) = \mathcal{N}(t|y(x,W), \Sigma)
\end{equation}

\begin{equation}
  y(x,W) = W^T \phi(x)
\end{equation}

likelihood function of multivariate target variable $t$ is:
\begin{gather}
  ln p(t|y(x,W),\Sigma) = -\frac{ND}{2} ln(2\pi) - \frac{N}{2} ln|\Sigma| - \frac{1}{2} \sum_{n=1}^{N} (t_n - y(x,W))^T\Sigma^{-1} (t_n - y(x,W)) \\
  = -\frac{ND}{2} ln(2\pi) - \frac{N}{2} ln|\Sigma| - \frac{1}{2} \sum_{n=1}^{N} (t_n - W^T \phi(x))^T\Sigma^{-1} (t_n - W^T \phi(x)) \\
  \frac{d}{d\Sigma} = -\frac{N}{2|\Sigma|} \frac{d}{d|\Sigma|} + \frac{1}{2}\sum_{n=1}^{N} (t_n - W^T \phi(x))^T\Sigma^{-1} I \Sigma^{-1} (t_n - W^T \phi(x)) \\
  \frac{N}{2} \Sigma^{-1} = \Sigma^{-1} \frac{1}{2}\sum_{n=1}^{N} (t_n - W^T \phi(x)) (t_n - W^T \phi(x))^T \Sigma^{-1} \\
  \Sigma = \frac{1}{N}\sum_{n=1}^{N} (t_n - W_{ML}^T \phi(x)) (t_n - W_{ML}^T \phi(x))^T
\end{gather}

Important notes learned from this exercise: tr scalar = scalar and $X^T y X$ is a scalar for any dimension compatible matrices. Also, tr(ABC) = tr(BCA) = tr(CAB)

\subsection{Exercise 4.8}
Not attempted

\subsection{Exercise 4.9}
Finding $E[L(t,f(x))]$ for multiple target variables where:
\begin{equation}
  E[L(t,f(x))] = \int \int ||f(x) - t||^2 p(x,t)dxdt
\end{equation}

And showing that the function f(x) that minimizes E is given by $E_t[t|x]$
\begin{gather}
  E[L(t,f(x))] = \int \int ||f(x) - t||^2 p(x,t)dxdt \\
  = \int \int ||f(x) -E[t|x] + E[t|x] - t||^2 p(x,t)dxdt \\
  = \int \int ||f(x) - E[t|x]||^2 + 2(f(x)-E[t|x])(E[t|x]-t) + ||E[t|x] - t||^2 p(x,t)dxdt \\
  \text{Integrating over t} \\
  \int ||f(x) - E[t|x]||^2 p(x)dx + \int var(t|x) p(x) dx \\ 
  \text{Minimizing this gives} \\
  f(x) = E[t|x]
\end{gather}

\subsection{Exercise 4.10}
Not attempted

\subsection{Exercise 4.11}
Normalizing

\begin{equation}
  p(x|\sigma^2, q) = \frac{q}{2(2\sigma^2)^\frac{1}{q} \Gamma(\frac{1}{q})} e^{-\frac{|x|^q}{2\sigma^2}}
\end{equation}

where
\begin{equation}
  \Gamma(x) = \int_{-\infty}^{\infty}u^{x-1}e^{-u}du
\end{equation}

\begin{gather}
  \Gamma(\frac{1}{q}) = \int_{-\infty}^{\infty}u^{\frac{1}{q}-1}e^{-u}du \\
  \Gamma(\frac{1}{q}) = \int_{-\infty}^{\infty}u^{\frac{1}{q}-1}e^{-u}du
\end{gather}

Not finished

\subsection{Exercise 4.12}
Not attempted

