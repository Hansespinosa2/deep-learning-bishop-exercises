\section{Probabilities}

\subsection*{Exercise 2.1}
Bayes rule
\begin{gather}
  P[C=1|T=1] = \frac{P[T=1|C=1]*P[C=1]}{P[T=1|C=1]*P[C=1] + P[T=1|C=0]*P[C=0]} \\
  P[C=1|T=1]= \frac{0.90*0.001}{0.90*0.001 + 0.03*0.999} = 0.0292
\end{gather}

Given that the test result was positive, there is a 2.92\% chance that you have cancer.

\subsection{Exercise 2.2}
Not attempted

\subsection*{Exercise 2.3}
\begin{gather}
  p(\mathbf{y}) = \int p_\mathbf{u},_\mathbf{v}(\mathbf{u},\mathbf{y}-\mathbf{u})d\mathbf{u} \\
  = \int p_\mathbf{u}(\mathbf{u})p_\mathbf{v}(\mathbf{y} - \mathbf{u})d\mathbf{u}
\end{gather}

\subsection{Exercise 2.4}
Not attempted

\subsection*{Exercise 2.5}
Exponential:
\begin{equation}
  p(x|\lambda) = \lambda e^{-\lambda x}
\end{equation}

Laplace: 
\begin{equation}
  p(x|\mu,\gamma) = \frac{1}{2\gamma} e^{-\frac{|x-\mu|}{\gamma}}
\end{equation}

Verifying that the exponential distribution is normalized: 

\begin{gather}
  p(x|\lambda) = \lambda e^{-\lambda x} \\
  \int_{0}^{\infty} \lambda e^{-\lambda x} = -e^{-\lambda x} \Big |_0^\infty = \frac{1}{e^\infty} + \frac{1}{e^0} \\
 = 1
\end{gather}

Verifying the laplace distribution:

\begin{equation}
  p(x|\mu,\gamma) = \frac{1}{2\gamma} e^{-\frac{|x-\mu|}{\gamma}} \\
\end{equation}


\begin{equation}
  \begin{cases}
    \frac{1}{2\gamma} e^{-\frac{x-\mu}{\gamma}} & \text{if } x \geq \mu \\
    \frac{1}{2\gamma} e^{-\frac{-x+\mu}{\gamma}} & \text{if } x < \mu
  \end{cases}
\end{equation}

\begin{gather}
  \int_{\mu}^{\infty}\frac{1}{2\gamma} e^{-\frac{x-\mu}{\gamma}} = \frac{1}{2} e^{-\frac{x-\mu}{\gamma}} \Big |_\mu^\infty = -\frac{1}{2}(e^{-\infty} - e^0) \\
  = \frac{1}{2} \\
  \int_{-\infty}^{\mu}\frac{1}{2\gamma} e^{-\frac{-x+\mu}{\gamma}} = \frac{1}{2} e^{-\frac{-x+\mu}{\gamma}} \Big |_{-\infty}^\mu = \frac{1}{2}(e^{0} - e^{-\infty}) \\
  = \frac{1}{2} \\ 
  \frac{1}{2} + \frac{1}{2} = 1
\end{gather}


\subsection{Exercise 2.6}
Not attempted

\subsection{Exercise 2.7}

\begin{gather}
  P(x|D) = \frac{1}{N}\sum_{n=1}^N \delta(x-x_n) \\
  E[f] = \int p(x)f(x)dx \\
  \text{Substituting:} \\
  E[f] = \int \frac{1}{N}\sum_{n=1}^N \delta(x-x_n) f(x) dx \\
  E[f] =  \frac{1}{N}\sum_{n=1}^N \int_{x_n- \varepsilon}^{x_n + \varepsilon} \delta(x-x_n) f(x) dx \\
  E[f] =  \frac{1}{N}\sum_{n=1}^N f(x_n) \int_{x_n- \varepsilon}^{x_n + \varepsilon} \delta(x-x_n) dx \\
  E[f] =  \frac{1}{N}\sum_{n=1}^N f(x_n)
\end{gather}

\subsection{Exercise 2.8}
Not attempted

\subsection{Exercise 2.9}

\begin{equation}
  cov[X,y] = E_{x,y}[xy] - E[x]E[y] \\
\end{equation}

If x and y are independent, the joint distribution is equal to the product of the marginals. $p(x,y) = p(x)p(y)$. If $E_{x,y}[xy] = E[x]E[y]$, then the covariance will be zero.

\subsection{Exercise 2.10}
Not attempted

\subsection{Exercise 2.11}
Proving $E[x] = E_y[E_x[x|y]]$:
\begin{gather}
  E_x[x|y] = \int p(x|y)xdx \\
  \text{Substituting:} \\
  E[x] = E_y[\int p(x|y)xdx] \\
  E[x] = \int E_y[p(x|y)]xdx \\
  E[x] = \int \int p(x|y)xp(y) dxdy \\
  E[x] = \int \int \frac{p(x,y)}{p(y)}xp(y) dxdy \\
  E[x] = \int \int p(x,y)x dxdy \\
  E[x] = E[x]
\end{gather}


\subsection{Exercise 2.12}
Not attempted

\subsection{Exercise 2.13}
