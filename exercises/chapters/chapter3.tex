\section{Standard Distributions}

\subsection{Exercise 3.1}
Proving $\sum_{x=0}^{1}p(x|\mu) = 1$: 
\begin{gather}
  Bern(x|\mu) = \mu^x(1-\mu)^{1-x} \\
  \sum_{x=0}^{1}p(x|\mu) = \mu^0(1-\mu)^1 + \mu^1(1-\mu)^0 \\
  = 1-\mu + \mu \\
   = 1
\end{gather}

Proving $E[x] = \mu$:
\begin{gather}
  E[x] = \sum_{x=0}^{1}p(x|\mu)*x \\
  E[x] = 0*\mu^0(1-\mu)^1 + 1 * \mu^1(1-\mu)^0 \\
  = 0*(1-\mu) + 1*\mu \\
  = \mu
\end{gather}

Proving $var[x] = \mu(1-\mu)$:
\begin{gather}
  Var[x] = \sum_{i}(x_i - \mu)^2p(x_i) \\
  = (0-\mu)^2(1-\mu) + (1-\mu)^2\mu \\
  = \mu^2(1-\mu) + (1-\mu)^2\mu \\
  = \mu(1-\mu)(\mu+(1-\mu)) \\
  = \mu(1-\mu)(1) \\
  = \mu(1-\mu)
\end{gather}

Solving for entropy of the bernoulli distribution proves simple as there are two probabilities, $p_1 = \mu, p_2 = 1-\mu$. Therefore the entropy must be:

\begin{equation}
  H[x] = -\mu ln \mu - (1-\mu)ln(1-\mu)
\end{equation}


\subsection{Exercise 3.2}
Not attempted

\subsection{Exercise 3.3}
Normalizing the binomial distribution ${N \choose m} \mu^m (1-\mu)^{N-m}$:
\begin{gather}
  \text{First step is proving } {N \choose m} + {N \choose m-1} = {N+1 \choose m}: \\
  {N \choose m} + {N \choose m-1} = \frac{N!}{(N-m)!m!} + \frac{N!}{(N-m+1)!(m-1)!} \\
  = \frac{N!}{(N-m)!m!} * \frac{N-m+1}{N-m+1} + \frac{N!}{(N-m+1)!(m-1)!} * \frac{m}{m} \\
  = \frac{N!(N+1-m)}{(N+1-m)!m!} + \frac{N!m}{(N-m+1)!m!} \\
  = \frac{(N+1)!-N!m + N!m}{(N+1-m)!m!} \\
  = \frac{(N+1)!}{(N+1-m)!m!} \\
  = {N+1 \choose m}
\end{gather}

With this information we would like to prove that $(1+x)^N = \sum_{m=0}^{N} {N \choose m} x^m$

N=2:
\begin{gather}
  {2 \choose 0} x^0 + {2 \choose 1} x^1 +  {2 \choose 2} x^2 = 1 + 2x + x^2 = (1+x)^2
\end{gather}

N=3:
\begin{gather}
  {3 \choose 0} x^0 + {3 \choose 1} x^1 +  {3 \choose 2} x^2 {3 \choose 3} x^3 = 1 + 3x + 3x^2 + x^3 = (1+x)^3
\end{gather}

N=4
\begin{gather}
  {4 \choose 0} x^0 + {4 \choose 1} x^1 +  {4 \choose 2} x^2 {4 \choose 3} x^3 + {4 \choose 4}x^4 = 1 + 4x + 6x^2 +4x^3 + x^4= (1+x)^4
\end{gather}

By induction:
$\forall x \in \mathbb{R} \text{ and } N \in \mathbb{N}$
\begin{gather}
  (1+x)^N = \sum_{m=0}^{N} {N \choose m} x^m
\end{gather}

Normalizing the binomial distribution:
\begin{gather}
  \sum_{m=0}^{N}{N \choose m} \mu^m (1-\mu)^{N-m} = 1 \\
  = (1-\mu)^N \sum_{m=0}^{N}{N \choose m} \mu^m (1-\mu)^{-m} \\
  = (1-\mu)^N \sum_{m=0}^{N}{N \choose m} (\frac{\mu}{1-\mu})^m \\
  = (1-\mu)^N (1+\frac{\mu}{1-\mu})^N \\
  = ((1-\mu)(1+\frac{\mu}{1-\mu}))^N \\
  = (1+\frac{\mu}{1-\mu}-\mu -\frac{\mu^2}{1-\mu}) \\
  = ((1-\mu) + \frac{\mu}{1-\mu}(1-\mu))^N \\
  = (1-\mu + \mu)^N \\
  = 1
\end{gather}

\subsection{Exercise 3.4}
Not attempted

\subsection{Exercise 3.5}
Finding the mode of the multivariate gaussian. The mode of a distribution is the same as the maximum of the probability density function. For the multivariate gaussian, this is:

\begin{equation}
  \mathcal{N}(\mathbf{x} | \mathbf{\mu}, \mathbf{\Sigma}) = \frac{1}{(2\pi)^{D/2}|\mathbf{\Sigma}|^\frac{1}{2}}e^{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})}
\end{equation}

\begin{gather}
  \frac{d}{d\mathbf{x}} \mathcal{N}(\mathbf{x} | \mathbf{\mu}, \mathbf{\Sigma}) \text{ ignoring normalization constant for brevity} \\
  0 = -\frac{1}{2}e^{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})} * \frac{d}{d\mathbf{x}}(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu}) \\
  0 = -\frac{1}{2}e^{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})} *2\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu}) \\
  \mathbf{x} = \mathbf{\mu}
\end{gather}

\subsection{Exercise 3.6}
Not attempted

\subsection{Exercise 3.7}
Finding Kullback-Leibler divergence between $q(\mathbf{x}) = \mathcal{N}(\mathbf{x} | \mathbf{\mu_q}, \mathbf{\Sigma_q})$ and $p(\mathbf{x}) = \mathcal{N}(\mathbf{x} | \mathbf{\mu_p}, \mathbf{\Sigma_p})$
\begin{equation}
  KL(p||q) = -\int p(x) ln{\frac{q(x)}{p(x)}}dx 
\end{equation}

\begin{gather}
  KL(q(x)||p(x)) = -\int \mathcal{N}(\mathbf{x} | \mathbf{\mu_q}, \mathbf{\Sigma_q}) ln {\frac{\mathcal{N}(\mathbf{x} | \mathbf{\mu_p}, \mathbf{\Sigma_p})}{\mathcal{N}(\mathbf{x} | \mathbf{\mu_q}, \mathbf{\Sigma_q})}} \\
  = -\int \mathcal{N}(\mathbf{x} | \mathbf{\mu_q}, \mathbf{\Sigma_q}) (ln \mathcal{N}(\mathbf{x} | \mathbf{\mu_p}, \mathbf{\Sigma_p}) - ln \mathcal{N}(\mathbf{x} | \mathbf{\mu_q}, \mathbf{\Sigma_q})) \\
  \text{definition: } ln \mathcal{N}(\mathbf{x} | \mathbf{\mu_p}, \mathbf{\Sigma_p}) = -\frac{D}{2}ln(2\pi) - \frac{1}{2} ln | \mathbf{\Sigma_p} | - \frac{1}{2} (\mathbf{x}-\mathbf{\mu_p})^T\mathbf{\Sigma_p}^{-1}(\mathbf{x}-\mathbf{\mu_p}) \\
  = - \frac{1}{2}\int \mathcal{N}(\mathbf{x} | \mathbf{\mu_q}, \mathbf{\Sigma_q}) (-ln |\mathbf{\Sigma_p}| + ln |\mathbf{\Sigma_q}| - (\mathbf{x}-\mathbf{\mu_p})^T\mathbf{\Sigma_p}^{-1}(\mathbf{x}-\mathbf{\mu_p}) + (\mathbf{x}-\mathbf{\mu_q})^T\mathbf{\Sigma_q}^{-1}(\mathbf{x}-\mathbf{\mu_q}) ) \\
  \text{Splitting integral into three} \\
  \text{First: } - \frac{1}{2} ln \frac{|\mathbf{\Sigma_q}|}{|\mathbf{\Sigma_p}|} \int \frac{1}{(2\pi)^{D/2}|\mathbf{\Sigma_q}|^\frac{1}{2}}e^{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu_q})^T\mathbf{\Sigma_q}^{-1}(\mathbf{x}-\mathbf{\mu_q})}  \\
  \text{Second: } \frac{1}{2}\int \frac{1}{(2\pi)^{D/2}|\mathbf{\Sigma_q}|^\frac{1}{2}}e^{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu_q})^T\mathbf{\Sigma_q}^{-1}(\mathbf{x}-\mathbf{\mu_q})} (\mathbf{x}-\mathbf{\mu_p})^T\mathbf{\Sigma_p}^{-1}(\mathbf{x}-\mathbf{\mu_p}) \\
  \text{Third: } -\frac{1}{2}\int  \frac{1}{(2\pi)^{D/2}|\mathbf{\Sigma_q}|^\frac{1}{2}}e^{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu_q})^T\mathbf{\Sigma_q}^{-1}(\mathbf{x}-\mathbf{\mu_q})} (\mathbf{x}-\mathbf{\mu_p})^T\mathbf{\Sigma_p}^{-1}(\mathbf{x}-\mathbf{\mu_p}) \\
  \text{Evaluating first integral: } - \frac{1}{2} ln \frac{|\mathbf{\Sigma_q}|}{|\mathbf{\Sigma_p}|} * 1 \\
  \text{Evaluating second integral: } \frac{1}{2}(\mathbf{\mu_p}-\mathbf{\mu_q})^T\mathbf{\Sigma_p}^{-1}(\mathbf{\mu_p}-\mathbf{\mu_q}) \\
  \text{Evaluating third integral: } - \frac{1}{2}(\mathbf{\mu_p}-\mathbf{\mu_q})^T\mathbf{\Sigma_p}^{-1}(\mathbf{\mu_p}-\mathbf{\mu_q}) + \frac{1}{2} Tr(\mathbf{\Sigma_p^{-1} \Sigma_q})
\end{gather}

I am not sure about this one it is quite hard
\subsection{Exercise 3.8}
Not attempted

\subsection{Exercise 3.9}
Not attempted

\subsection{Exercise 3.10}
Not attempted

\subsection{Exercise 3.11}
Not attempted

\subsection{Exercise 3.12}
Not attempted

\subsection{Exercise 3.13}
Not attempted

\subsection{Exercise 3.14}
Not attempted

\subsection{Exercise 3.15}
Not attempted

\subsection{Exercise 3.16}
Not attempted

\subsection{Exercise 3.17}
Not attempted

\subsection{Exercise 3.18}
Not attempted

\subsection{Exercise 3.19}
Not attempted

\subsection{Exercise 3.20}
Not attempted

\subsection{Exercise 3.21}
Not attempted

\subsection{Exercise 3.22}
Not attempted

\subsection{Exercise 3.23}
Not attempted

\subsection{Exercise 3.24}
Not attempted

\subsection{Exercise 3.25}
Not attempted

\subsection{Exercise 3.26}
Not attempted

\subsection{Exercise 3.27}
Not attempted

\subsection{Exercise 3.28}
Not attempted

\subsection{Exercise 3.29}
Not attempted

\subsection{Exercise 3.30}
Not attempted

\subsection{Exercise 3.31}
Not attempted

\subsection{Exercise 3.32}
Not attempted

\subsection{Exercise 3.33}
Not attempted

\subsection{Exercise 3.34}
Not attempted

\subsection{Exercise 3.35}
Not attempted

\subsection{Exercise 3.36}
Not attempted

\subsection{Exercise 3.37}
Deriving a maximimum likelihood estimator for a histogram-like density model. Space $\mathbf{x}$ is divided into fixed regions with density $p(\mathbf{x}) = h_i \text{ }\forall\text{ } i \text{ regions}$. Volume $i$ is denoted $\Delta_i$. $N$ total observations such that $n_i$ observations are in region $i$.
\begin{gather}
  h_i = \frac{n_i}{N\Delta_i} \\
  \sum_i h_i\Delta_i = 1 \\
  L = \prod_{i=0}^{K} h_i^{n_i} \\
  log L = \prod_{i=0}^{K} n_i log h_i \\
  max \sum_{i=0}^{K} n_i log h_i - \lambda(\sum_i h_i\Delta_i - 1) \\ 
  0 = \frac{n_i}{h_i} - \lambda\Delta_i \\
  \frac{n_i}{h_i} = \lambda \Delta_i \\ 
  h_i = \frac{n_i}{\lambda \Delta_i}\\
  \text{Plugging into normalization:} \\
  \sum_i \frac{n_i}{\lambda} = 1  \\
  \lambda = N \\
  h_i = \frac{n_i}{N\Delta_i}
\end{gather}

\subsection{Exercise 3.38}
Not attempted